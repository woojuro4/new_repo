{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWl_TZt2VsnQ"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_GAaanFVqgF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zo-wWEvgVqgJ"
   },
   "outputs": [],
   "source": [
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZSRCMipVqgN"
   },
   "source": [
    "### 과제 1\n",
    "\n",
    "##### 강의 중 배운 내용 몇가지를 직접 구현해보자!\n",
    "\n",
    "1. Weight initialization \n",
    "  * Xavier initialization을 직접 구현해보세요. [모르겠다면 참조](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)\n",
    "\n",
    "\n",
    "##### 도전 정신이 뛰어나신 분들을 위한 과제... (안해도 됩니다)\n",
    "\n",
    "2. Regularizer (선택)\n",
    "    * Regularizer를 직접 구현해보세요. 역전파를 위한 regularization이니 이 점 꼭 기억하세요 [참조](https://jamesmccaffrey.wordpress.com/2017/06/27/implementing-neural-network-l1-regularization/)\n",
    "\n",
    "3. Activation (선택)\n",
    "    * 활성화 함수를 직접 구현해보세요. 활성화 함수에 따라서 gradient backpropagation 방식이 달라집니다. 활성화 함수를 추가한다면 gradient를 구하는 공식도 활성화 함수에 맞게 구현해야 합니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYqNRxBCVqgO"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "(train_X, train_y) , (test_X, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# MLP를 위한 shape 조정\n",
    "train_X, test_X = train_X.reshape((-1, 28*28)), test_X.reshape((-1,28*28))\n",
    "\n",
    "# 원활한 학습을 위한 scaling\n",
    "train_X = MinMaxScaler().fit_transform(train_X)\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gS60-xgBVqgR"
   },
   "outputs": [],
   "source": [
    "class mlp():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.classes = len(set(self.y))\n",
    "        \n",
    "        # weight initialization (random initialization from normal distribution)\n",
    "        self.W = self.initialize(self.X.shape[1],self.classes)\n",
    "        self.b = np.zeros((1,self.classes))\n",
    "                \n",
    "        # your code goes here\n",
    "        \n",
    "    def regularizer(self,A):\n",
    "        # your code goes here\n",
    "        # (example given is l2 regularization)\n",
    "        return self.reg*A                    #지우고 작성하세요\n",
    "    \n",
    "    def initialize(self,m,h):\n",
    "        # your code goes here\n",
    "        init = np.random.normal(size=(m,h))  #지우고 작성하세요\n",
    "        return init\n",
    "    \n",
    "    def activation(self, A):\n",
    "        # your code goes here\n",
    "        return A                             #지우고 작성하세요\n",
    "    \n",
    "    def softmax(self, A):\n",
    "        return np.exp(A) / np.exp(A).sum()\n",
    "    \n",
    "    def feedForward(self, X):\n",
    "        # 아래는 softmax를 axis=1(주어진 batch에서 각 데이터별로) softmax를 취하는 함수 입니다.\n",
    "        return np.apply_along_axis(self.softmax, -1, self.activation(np.matmul(X, self.W) + self.b))\n",
    "    \n",
    "    def train(self, lr, iteration, batch_size, reg=0.01):\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_hist = []\n",
    "        self.W_hist = []\n",
    "        self.b_hist = []\n",
    "        \n",
    "        for epoch in range(iteration):\n",
    "            print(f\"Epoch {epoch + 1}\")\n",
    "            loss, W, b = self.gd()\n",
    "            print(f\"Loss : {loss[-1]}\")\n",
    "            self.loss_hist.extend(loss)\n",
    "            self.W_hist.extend(W)\n",
    "            self.b_hist.extend(b)\n",
    "            \n",
    "        self.param = (self.W_hist[-1], self.b_hist[-1])\n",
    "        self.losses = (self.loss_hist[-1])\n",
    "        return self.param, self.losses\n",
    "    \n",
    "    def gd(self):\n",
    "        loss_list = []\n",
    "        W_list = []\n",
    "        b_list = []\n",
    "        \n",
    "        x_batches = np.split(self.X, len(self.X)/self.batch_size)\n",
    "        y_batches = np.split(self.y, len(self.X)/self.batch_size)\n",
    "        \n",
    "        for inp, tar in zip(x_batches,y_batches):\n",
    "            y_hat = self.feedForward(inp)\n",
    "            self.loss = (-np.log(y_hat[np.eye(10)[tar].astype(bool)])).mean()\n",
    "            loss_list.append(self.loss)\n",
    "            \n",
    "            # softmax classifier의 gradient 함수\n",
    "            self.gradient = (y_hat - np.eye(10)[tar]) / tar.shape[0]\n",
    "            \n",
    "            # backpropagation for Weights\n",
    "            dW = np.dot(inp.T, self.gradient) + self.regularizer(self.W)\n",
    "            self.W -= self.lr*(dW)\n",
    "            W_list.append(self.W)\n",
    "            \n",
    "            # backpropagation for biases\n",
    "            db = self.gradient.sum(axis=0, keepdims=True)\n",
    "            self.b -= self.lr*(db)\n",
    "            b_list.append(self.b)\n",
    "            \n",
    "        return loss_list, W_list, b_list\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        return self.feedForward(X)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCxx-gQ8VqgU"
   },
   "outputs": [],
   "source": [
    "model = mlp(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMiAFZVpVqgX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(W, b), loss = model.train(lr=0.01, iteration=5, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3YDE04lVqga",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(model.loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4syACmTVqgf"
   },
   "source": [
    "### 과제 2\n",
    "\n",
    "##### tensorflow를 활용해서 MLP모델을 편하게 설계 해보자!\n",
    "\n",
    "1. 데이터 선택\n",
    "    * 모델링 해보고 싶은 데이터 선택\n",
    "    * 데이터를 활용해 간단한 EDA를 실행해보시길 권장드립니다\n",
    "2. MLP 설계\n",
    "    * 원하는 구조로 MLP 모델 설계\n",
    "3. 성능 확인 및 parameter 조정\n",
    "    * 성능을 확인하고, 성능개선을 위해 parameter 조정\n",
    "    * 고려해볼 parameter 조정 : \n",
    "        * Number of layers\n",
    "        * Number of hidden units\n",
    "        * Activation function\n",
    "        * Batch Size\n",
    "        * Training Epochs \n",
    "        * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "405f2a13Vqgg"
   },
   "source": [
    "#### 간단한 데이터 설명\n",
    "- Boston Housing 데이터: \n",
    "    * 13개의 연속형 변수 항목으로 해당 지역의 집값 (중위값)을 구하는 TASK\n",
    "- MNIST 데이터:\n",
    "    * 28 X 28의 픽셀 값으로 구성되어 있으며 해당 그림이 어느 숫자인지 분류하는 TASK\n",
    "- CIFAR 10 데이터:\n",
    "    * 32 X 32 X 3의 픽셀 값으로 구성되어 있으며 10가지의 항목 중 어느 항목에 해당하는 사진인지 분류하는 TASK\n",
    "- 도전해보고 싶은 데이터 외부에서 가져와도 됩니다!\n",
    "\n",
    "*데이터에 대해 더 자세히 알고 싶다면 https://keras.io/api/datasets/ 참조*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjHG9AKsVqgh"
   },
   "source": [
    "#### 몇가지 가이드라인 코드입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ecuBpN2Vqgh"
   },
   "outputs": [],
   "source": [
    "# MLP모델의 학습 과정을 시각화를 위한 함수\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    for e, (item, value) in enumerate(history.history.items()):\n",
    "        plt.subplot(2,2,e+1)\n",
    "        plt.plot(value)\n",
    "        plt.title(item)\n",
    "    plt.show()\n",
    "    \n",
    "# 사용법\n",
    "# history = model.fit(...)\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NklR31z0Vqgk"
   },
   "outputs": [],
   "source": [
    "# tensorflow parameters\n",
    "\n",
    "activation_list = [\"sigmoid\", \"relu\", \"softmax\", \"tanh\"]\n",
    "\n",
    "loss_list = [\"sparse_categorical_crossentropy\",\n",
    "             \"categorical_crossentropy\", \n",
    "             \"binary_crossentropy\"]\n",
    "\n",
    "optimizer_list = [\"sgd\", \"adam\", \"rmsprop\", \"adagrad\"]\n",
    "\n",
    "initializer_list = [tf.keras.initializers.RandomNormal(), \n",
    "                    tf.keras.initializers.RandomUniform(), \n",
    "                    tf.keras.initializers.he_normal(), \n",
    "                    tf.keras.initializers.he_uniform(), \n",
    "                    tf.keras.initializers.GlorotUniform(),\n",
    "                    tf.keras.initializers.GlorotNormal()]\n",
    "\n",
    "# dropout\n",
    "dropout_rate = 0.3\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation = \"sigmoid\"),\n",
    "    tf.keras.layers.Dense(2, activation = \"sigmoid\"),\n",
    "    tf.keras.layers.Dropout(dropout_rate)\n",
    "])\n",
    "\n",
    "\n",
    "# regularizer\n",
    "regularizer = tf.keras.regularizers.l1(1e-3)\n",
    "regularizer = tf.keras.regularizers.l2(1e-3)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation=\"sigmoid\",\n",
    "                          activity_regularizer=regularizer)\n",
    "])\n",
    "\n",
    "# weight initialization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=784, activation=\"sigmoid\",\n",
    "                          kernel_initializer=initializer_list[0])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijWAQmYsVqgm"
   },
   "source": [
    "### 데이터 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7FPPbUmMVqgn"
   },
   "outputs": [],
   "source": [
    "### Boston Housing Prediction\n",
    "(train_X, train_y) , (test_X, test_y) = tf.keras.datasets.boston_housing.load_data()\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py0LWuIBVqgq"
   },
   "outputs": [],
   "source": [
    "### MNIST 분류\n",
    "(train_X, train_y) , (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_X, test_X = train_X.reshape((-1, 28*28)), test_X.reshape((-1, 28*28))\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GG4oIWV0Vqgt"
   },
   "outputs": [],
   "source": [
    "### CIFAR 10 분류\n",
    "(train_X, train_y) , (test_X, test_y) = tf.keras.datasets.cifar10.load_data()\n",
    "train_X, test_X = train_X.reshape((-1, train_X.shape[1]**2)), test_X.reshape((-1, train_X.shape[1]**2))\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UyDJr0JVqgw"
   },
   "source": [
    "### EDA (권장) \n",
    "\n",
    "* 필요에 따른 EDA를 실시하세요 ! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbX3d8PJVqgw"
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2lQ7K24Vqgz"
   },
   "source": [
    "## 모델링\n",
    "\n",
    "* 다양하게 시도해보세요! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LgkfEi2Vqgz"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # your code goes here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tO4G0EN-Vqg2"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = # your code goes here,\n",
    "              optimizer = # your code goes here, \n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWSk_KUBVqg5"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, \n",
    "                    epochs = # your code goes here, \n",
    "                    batch_size = # your code goes here, \n",
    "                    validation_split = # your code goes here, \n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNijqGXCVqg7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72a-5STDVqg-"
   },
   "outputs": [],
   "source": [
    "print(\"Test Loss: {0}, Test Accuracy: {1}\".format(*model.evaluate(test_X, test_y, batch_size=8)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL기초-과제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
